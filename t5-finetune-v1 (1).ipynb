{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-14T11:53:04.465678Z","iopub.execute_input":"2023-06-14T11:53:04.466553Z","iopub.status.idle":"2023-06-14T11:53:04.484246Z","shell.execute_reply.started":"2023-06-14T11:53:04.466508Z","shell.execute_reply":"2023-06-14T11:53:04.482997Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/squad-t5/squad_GPT_Neo_train.csv\n/kaggle/input/squad-t5/squad_GPT_Neo_val.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install datasets\n!pip install tqdm\n!pip install --quiet transformers\n!pip install --quiet sentencepiece\n!pip install --quiet pytorch-lightning\n!pip install --quiet torchtext\n! pip install --upgrade accelerate","metadata":{"execution":{"iopub.status.busy":"2023-06-14T11:53:04.487155Z","iopub.execute_input":"2023-06-14T11:53:04.487538Z","iopub.status.idle":"2023-06-14T11:54:26.839734Z","shell.execute_reply.started":"2023-06-14T11:53:04.487505Z","shell.execute_reply":"2023-06-14T11:54:26.838609Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.23.5)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (10.0.1)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (1.5.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.28.2)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.64.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.5.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.4)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.14.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (2.1.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.12.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (5.4.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.5.7)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.64.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.12.0)\nCollecting accelerate\n  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.4.1)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.0.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (3.12.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\nInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.12.0\n    Uninstalling accelerate-0.12.0:\n      Successfully uninstalled accelerate-0.12.0\nSuccessfully installed accelerate-0.20.3\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/squad-t5/squad_GPT_Neo_train.csv\")\nvalid = pd.read_csv(\"/kaggle/input/squad-t5/squad_GPT_Neo_val.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-06-14T11:54:26.842324Z","iopub.execute_input":"2023-06-14T11:54:26.842951Z","iopub.status.idle":"2023-06-14T11:54:28.236826Z","shell.execute_reply.started":"2023-06-14T11:54:26.842908Z","shell.execute_reply":"2023-06-14T11:54:28.235883Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Check we have a GPU and check the memory size of the GUP\n!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2023-06-14T11:54:28.238331Z","iopub.execute_input":"2023-06-14T11:54:28.238663Z","iopub.status.idle":"2023-06-14T11:54:29.212863Z","shell.execute_reply.started":"2023-06-14T11:54:28.238628Z","shell.execute_reply":"2023-06-14T11:54:29.211697Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Wed Jun 14 11:54:29 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   37C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"train_file_path = '/kaggle/input/squad-t5/squad_GPT_Neo_train.csv'\nvalidation_file_path = '/kaggle/input/squad-t5/squad_GPT_Neo_val.csv'","metadata":{"execution":{"iopub.status.busy":"2023-06-14T11:54:29.216252Z","iopub.execute_input":"2023-06-14T11:54:29.216669Z","iopub.status.idle":"2023-06-14T11:54:29.221932Z","shell.execute_reply.started":"2023-06-14T11:54:29.216624Z","shell.execute_reply":"2023-06-14T11:54:29.220881Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import argparse\nimport glob\nimport os\nimport json\nimport time\nimport logging\nimport random\nimport re\nfrom itertools import chain\nfrom string import punctuation\n\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport pytorch_lightning as pl\nfrom termcolor import colored\nimport textwrap\n\nfrom transformers import (\n    AdamW,\n    T5ForConditionalGeneration,\n    T5Tokenizer,\n    get_linear_schedule_with_warmup\n)\n\npl.seed_everything(42)","metadata":{"execution":{"iopub.status.busy":"2023-06-14T11:54:29.223359Z","iopub.execute_input":"2023-06-14T11:54:29.223765Z","iopub.status.idle":"2023-06-14T11:54:42.169204Z","shell.execute_reply.started":"2023-06-14T11:54:29.223734Z","shell.execute_reply":"2023-06-14T11:54:42.168204Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"42"},"metadata":{}}]},{"cell_type":"code","source":"t5_tokenizer = T5Tokenizer.from_pretrained('t5-base')\nt5_model = T5ForConditionalGeneration.from_pretrained('t5-base')","metadata":{"execution":{"iopub.status.busy":"2023-06-14T12:00:29.715867Z","iopub.execute_input":"2023-06-14T12:00:29.716403Z","iopub.status.idle":"2023-06-14T12:00:36.454018Z","shell.execute_reply.started":"2023-06-14T12:00:29.716360Z","shell.execute_reply":"2023-06-14T12:00:36.453133Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11d1f1b4e23442af9870180ff6c00df7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d131da309ff4b68a7b4f0318c93ff7d"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\nFor now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90642f429b66488ebe0cd771550f5dff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0884ef7cb5a1434d91da16d8f943ba5d"}},"metadata":{}}]},{"cell_type":"code","source":"from pprint import pprint\nsample_encoding = t5_tokenizer.encode_plus(\"This is for testing\",\n                                        max_length=64,\n                                        pad_to_max_length=True,\n                                        truncation=True,\n                                        return_tensors=\"pt\")\n\nprint (sample_encoding.keys())\npprint (sample_encoding)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T05:33:41.115326Z","iopub.execute_input":"2023-06-10T05:33:41.115758Z","iopub.status.idle":"2023-06-10T05:33:41.130350Z","shell.execute_reply.started":"2023-06-10T05:33:41.115725Z","shell.execute_reply":"2023-06-10T05:33:41.129499Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"dict_keys(['input_ids', 'attention_mask'])\n{'attention_mask': tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n 'input_ids': tensor([[ 100,   19,   21, 2505,    1,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0]])}\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"print (sample_encoding['input_ids'].shape)\nprint (sample_encoding['input_ids'].squeeze().shape)\nprint (sample_encoding['input_ids'])","metadata":{"execution":{"iopub.status.busy":"2023-06-10T05:33:41.131713Z","iopub.execute_input":"2023-06-10T05:33:41.132058Z","iopub.status.idle":"2023-06-10T05:33:41.138017Z","shell.execute_reply.started":"2023-06-10T05:33:41.132028Z","shell.execute_reply":"2023-06-10T05:33:41.137144Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"torch.Size([1, 64])\ntorch.Size([64])\ntensor([[ 100,   19,   21, 2505,    1,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0]])\n","output_type":"stream"}]},{"cell_type":"code","source":"# In sentencepiece when joining to get back a sentence replace _ by space.\ntokenized_output = t5_tokenizer.convert_ids_to_tokens(sample_encoding['input_ids'].squeeze())\nprint (tokenized_output)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-10T05:33:41.139350Z","iopub.execute_input":"2023-06-10T05:33:41.139912Z","iopub.status.idle":"2023-06-10T05:33:41.148611Z","shell.execute_reply.started":"2023-06-10T05:33:41.139881Z","shell.execute_reply":"2023-06-10T05:33:41.147643Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"['▁This', '▁is', '▁for', '▁testing', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n","output_type":"stream"}]},{"cell_type":"code","source":"decoded_output = t5_tokenizer.decode(sample_encoding['input_ids'].squeeze(), skip_special_tokens=True,clean_up_tokenization_spaces=True)\nprint (decoded_output)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T05:33:41.150141Z","iopub.execute_input":"2023-06-10T05:33:41.150460Z","iopub.status.idle":"2023-06-10T05:33:41.184010Z","shell.execute_reply.started":"2023-06-10T05:33:41.150430Z","shell.execute_reply":"2023-06-10T05:33:41.183078Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"This is for testing\n","output_type":"stream"}]},{"cell_type":"code","source":"from tqdm.notebook import tqdm\nimport copy\n\nclass QuestionGenerationDataset(Dataset):\n    def __init__(self, tokenizer, filepath, max_len_inp=512,max_len_out=96):\n        self.path = filepath\n\n        self.passage_column = \"context\"\n        self.answer = \"answer\"\n        self.question = \"question\"\n\n        # self.data = pd.read_csv(self.path)\n        self.data = pd.read_csv(self.path)\n\n        self.max_len_input = max_len_inp\n        self.max_len_output = max_len_out\n        self.tokenizer = tokenizer\n        self.inputs = []\n        self.targets = []\n        self.skippedcount =0\n        self._build()\n\n    def __len__(self):\n        return len(self.inputs)\n  \n    def __getitem__(self, index):\n        source_ids = self.inputs[index][\"input_ids\"].squeeze()\n        target_ids = self.targets[index][\"input_ids\"].squeeze()\n\n        src_mask = self.inputs[index][\"attention_mask\"].squeeze()  # might need to squeeze\n        target_mask = self.targets[index][\"attention_mask\"].squeeze()  # might need to squeeze\n\n        labels = copy.deepcopy(target_ids)\n        labels [labels==0] = -100\n\n        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask,\"labels\":labels}\n\n    def _build(self):\n        for idx in tqdm(range(len(self.data))):\n            passage,answer,target = self.data.loc[idx, self.passage_column],self.data.loc[idx, self.answer], self.data.loc[idx, self.question]\n\n            input_ = \"context: %s  answer: %s </s>\" % (passage, answer)\n            target = \"question: %s </s>\" % (str(target))\n\n            # get encoding length of input. If it is greater than self.max_len skip it\n            test_input_encoding = self.tokenizer.encode_plus(input_,\n                                        truncation=False,\n                                        return_tensors=\"pt\")\n            \n            length_of_input_encoding = len(test_input_encoding['input_ids'][0])\n\n\n            if length_of_input_encoding > self.max_len_input:\n              self.skippedcount = self.skippedcount + 1\n              continue\n\n            # tokenize inputs\n            tokenized_inputs = self.tokenizer.batch_encode_plus(\n                [input_], max_length=self.max_len_input, pad_to_max_length=True, return_tensors=\"pt\"\n            )\n            # tokenize targets\n            tokenized_targets = self.tokenizer.batch_encode_plus(\n                [target], max_length=self.max_len_output, pad_to_max_length=True,return_tensors=\"pt\"\n            )\n\n            self.inputs.append(tokenized_inputs)\n            self.targets.append(tokenized_targets)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-10T05:33:41.185354Z","iopub.execute_input":"2023-06-10T05:33:41.185690Z","iopub.status.idle":"2023-06-10T05:33:41.200704Z","shell.execute_reply.started":"2023-06-10T05:33:41.185658Z","shell.execute_reply":"2023-06-10T05:33:41.199822Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"train_dataset = QuestionGenerationDataset(t5_tokenizer,train_file_path)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T05:33:41.205769Z","iopub.execute_input":"2023-06-10T05:33:41.206060Z","iopub.status.idle":"2023-06-10T05:38:30.374236Z","shell.execute_reply.started":"2023-06-10T05:33:41.206037Z","shell.execute_reply":"2023-06-10T05:38:30.373112Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/72103 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fba7bb830d4d4d6e86ee5e641c73d4e6"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:226: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\nToken indices sequence length is longer than the specified maximum sequence length for this model (598 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"}]},{"cell_type":"code","source":"train_sample = train_dataset[50]\ndecoded_train_input = t5_tokenizer.decode(train_sample['source_ids'])\ndecoded_train_output = t5_tokenizer.decode(train_sample['target_ids'])\n\nprint (decoded_train_input)\nprint (decoded_train_output)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T05:38:30.375739Z","iopub.execute_input":"2023-06-10T05:38:30.376312Z","iopub.status.idle":"2023-06-10T05:38:30.409877Z","shell.execute_reply.started":"2023-06-10T05:38:30.376276Z","shell.execute_reply":"2023-06-10T05:38:30.408975Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"context: After HMS Sheffield was wrecked by an Argentinian attack, The Sun was heavily criticised and even mocked for its coverage of the war in The Daily Mirror and The Guardian, and the wider media queried the veracity of official information and worried about the number of casualties, The Sun gave its response. \"There are traitors in our midst\", wrote leader writer Ronald Spark on 7 May, accusing commentators on Daily Mirror and The Guardian, plus the BBC's defence correspondent Peter Snow, of \"treason\" for aspects of their coverage. answer: Peter Snow</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\nquestion: Who was the BBC's defense correspondent?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","output_type":"stream"}]},{"cell_type":"code","source":"validation_dataset = QuestionGenerationDataset(t5_tokenizer,validation_file_path)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T05:38:30.411244Z","iopub.execute_input":"2023-06-10T05:38:30.411764Z","iopub.status.idle":"2023-06-10T05:39:07.297727Z","shell.execute_reply.started":"2023-06-10T05:38:30.411732Z","shell.execute_reply":"2023-06-10T05:39:07.296771Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/8850 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb41dbf2764443cba7b4a7e93492f50a"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:226: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"train_sample = train_dataset[50]\ndecoded_train_input = t5_tokenizer.decode(train_sample['source_ids'])\ndecoded_train_output = t5_tokenizer.decode(train_sample['target_ids'])\n\nprint (decoded_train_input)\nprint (decoded_train_output)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T05:39:07.299089Z","iopub.execute_input":"2023-06-10T05:39:07.300157Z","iopub.status.idle":"2023-06-10T05:39:07.332967Z","shell.execute_reply.started":"2023-06-10T05:39:07.300123Z","shell.execute_reply":"2023-06-10T05:39:07.331946Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"context: After HMS Sheffield was wrecked by an Argentinian attack, The Sun was heavily criticised and even mocked for its coverage of the war in The Daily Mirror and The Guardian, and the wider media queried the veracity of official information and worried about the number of casualties, The Sun gave its response. \"There are traitors in our midst\", wrote leader writer Ronald Spark on 7 May, accusing commentators on Daily Mirror and The Guardian, plus the BBC's defence correspondent Peter Snow, of \"treason\" for aspects of their coverage. answer: Peter Snow</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\nquestion: Who was the BBC's defense correspondent?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n","output_type":"stream"}]},{"cell_type":"code","source":"class T5FineTuner(pl.LightningModule):\n    def __init__(self, t5model, t5tokenizer):\n        super(T5FineTuner, self).__init__()\n#         self.hparams = hparams\n        self.model = t5model\n        self.tokenizer = t5tokenizer\n\n\n    def forward( self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, lm_labels=None):\n         outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            decoder_attention_mask=decoder_attention_mask,\n            labels=lm_labels,\n        )\n         \n         return outputs\n\n\n    def training_step(self, batch, batch_idx):\n        outputs = self.forward(\n            input_ids=batch[\"source_ids\"],\n            attention_mask=batch[\"source_mask\"],\n            decoder_input_ids = batch[\"target_ids\"],\n            decoder_attention_mask=batch['target_mask'],\n            lm_labels=batch['labels']\n        )\n\n        loss = outputs[0]\n        self.log('train_loss',loss)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        outputs = self.forward(\n            input_ids=batch[\"source_ids\"],\n            attention_mask=batch[\"source_mask\"],\n            decoder_input_ids = batch[\"target_ids\"],\n            decoder_attention_mask=batch['target_mask'],\n            lm_labels=batch['labels']\n        )\n\n        loss = outputs[0]\n        self.log(\"val_loss\",loss)\n        return loss\n\n    def train_dataloader(self):\n        return DataLoader(train_dataset, batch_size=4,num_workers=4)\n\n    def val_dataloader(self):\n        return DataLoader(validation_dataset, batch_size=4,num_workers=4)\n\n\n\n    def configure_optimizers(self):\n        optimizer = AdamW(self.parameters(), lr=3e-4, eps=1e-8)\n        return optimizer\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-06-10T05:39:07.334355Z","iopub.execute_input":"2023-06-10T05:39:07.334687Z","iopub.status.idle":"2023-06-10T05:39:07.345819Z","shell.execute_reply.started":"2023-06-10T05:39:07.334656Z","shell.execute_reply":"2023-06-10T05:39:07.344827Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"args_dict = dict(\n    batch_size =4,\n)\n\nargs = argparse.Namespace(**args_dict)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T05:39:07.347550Z","iopub.execute_input":"2023-06-10T05:39:07.347975Z","iopub.status.idle":"2023-06-10T05:39:07.357119Z","shell.execute_reply.started":"2023-06-10T05:39:07.347942Z","shell.execute_reply":"2023-06-10T05:39:07.356253Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"model = T5FineTuner(t5_model,t5_tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T05:39:07.358802Z","iopub.execute_input":"2023-06-10T05:39:07.359219Z","iopub.status.idle":"2023-06-10T05:39:07.366154Z","shell.execute_reply.started":"2023-06-10T05:39:07.359189Z","shell.execute_reply":"2023-06-10T05:39:07.365192Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# trainer = pl.Trainer(max_epochs = 1, gpus=1,progress_bar_refresh_rate=30)\ntrainer = pl.Trainer(max_epochs=1, accelerator='gpu')\n","metadata":{"execution":{"iopub.status.busy":"2023-06-10T05:39:07.367723Z","iopub.execute_input":"2023-06-10T05:39:07.368099Z","iopub.status.idle":"2023-06-10T05:39:08.553599Z","shell.execute_reply.started":"2023-06-10T05:39:07.368042Z","shell.execute_reply":"2023-06-10T05:39:08.552603Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"trainer.fit(model)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T05:40:19.170826Z","iopub.execute_input":"2023-06-10T05:40:19.171785Z","iopub.status.idle":"2023-06-10T07:29:00.249269Z","shell.execute_reply.started":"2023-06-10T05:40:19.171740Z","shell.execute_reply":"2023-06-10T07:29:00.247454Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c987496243142738a492102f25699c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}]},{"cell_type":"code","source":"print (\"Saving model\")\nsave_path_model = '/kaggle/working/t5/model/'\nsave_path_tokenizer = '/kaggle/working/t5/tokenizer/'","metadata":{"execution":{"iopub.status.busy":"2023-06-10T07:31:53.574674Z","iopub.execute_input":"2023-06-10T07:31:53.575079Z","iopub.status.idle":"2023-06-10T07:31:53.581943Z","shell.execute_reply.started":"2023-06-10T07:31:53.575046Z","shell.execute_reply":"2023-06-10T07:31:53.581049Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Saving model\n","output_type":"stream"}]},{"cell_type":"code","source":"model.model.save_pretrained(save_path_model)\nt5_tokenizer.save_pretrained(save_path_tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T07:32:01.089675Z","iopub.execute_input":"2023-06-10T07:32:01.090062Z","iopub.status.idle":"2023-06-10T07:32:02.125813Z","shell.execute_reply.started":"2023-06-10T07:32:01.090033Z","shell.execute_reply":"2023-06-10T07:32:02.124779Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/t5/tokenizer/tokenizer_config.json',\n '/kaggle/working/t5/tokenizer/special_tokens_map.json',\n '/kaggle/working/t5/tokenizer/spiece.model',\n '/kaggle/working/t5/tokenizer/added_tokens.json')"},"metadata":{}}]},{"cell_type":"markdown","source":"# Testing model ","metadata":{}},{"cell_type":"code","source":"trained_model_path = '/kaggle/working/t5/model/'\ntrained_tokenizer = '/kaggle/working/t5/tokenizer/'","metadata":{"execution":{"iopub.status.busy":"2023-06-10T07:45:23.354859Z","iopub.execute_input":"2023-06-10T07:45:23.355326Z","iopub.status.idle":"2023-06-10T07:45:23.360583Z","shell.execute_reply.started":"2023-06-10T07:45:23.355294Z","shell.execute_reply":"2023-06-10T07:45:23.359500Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"model = T5ForConditionalGeneration.from_pretrained(trained_model_path)\ntokenizer = T5Tokenizer.from_pretrained(trained_tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T07:45:23.541709Z","iopub.execute_input":"2023-06-10T07:45:23.542034Z","iopub.status.idle":"2023-06-10T07:45:30.227223Z","shell.execute_reply.started":"2023-06-10T07:45:23.542008Z","shell.execute_reply":"2023-06-10T07:45:30.226223Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint (\"device \",device)\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T07:45:30.229301Z","iopub.execute_input":"2023-06-10T07:45:30.229662Z","iopub.status.idle":"2023-06-10T07:45:30.455387Z","shell.execute_reply.started":"2023-06-10T07:45:30.229630Z","shell.execute_reply":"2023-06-10T07:45:30.454486Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"device  cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"context =\"The university first offered a master of arts in 1854–1855. Formal requirements developed for graduate degrees, including offering doctorate (phd) degrees - today each of the five colleges offer grad education, with most departments offering phd programs, while others offer doctorate degrees. The school of architecture offers 'master of architecture' while each department of college of engineering offers phd program. All departments in the college of science offer phds, except for the department of pre-professional studies.\"\nanswer = \"phds\"\ntext = \"context: \"+context + \" \" + \"answer: \" + answer + \" </s>\"\nprint (text)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T07:45:30.456962Z","iopub.execute_input":"2023-06-10T07:45:30.457340Z","iopub.status.idle":"2023-06-10T07:45:30.463472Z","shell.execute_reply.started":"2023-06-10T07:45:30.457306Z","shell.execute_reply":"2023-06-10T07:45:30.462414Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"context: The university first offered a master of arts in 1854–1855. Formal requirements developed for graduate degrees, including offering doctorate (phd) degrees - today each of the five colleges offer grad education, with most departments offering phd programs, while others offer doctorate degrees. The school of architecture offers 'master of architecture' while each department of college of engineering offers phd program. All departments in the college of science offer phds, except for the department of pre-professional studies. answer: phds </s>\n","output_type":"stream"}]},{"cell_type":"code","source":"encoding = tokenizer.encode_plus(text,max_length =512, padding=True, return_tensors=\"pt\")\nprint (encoding.keys())\ninput_ids,attention_mask  = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T07:45:30.466324Z","iopub.execute_input":"2023-06-10T07:45:30.466675Z","iopub.status.idle":"2023-06-10T07:45:30.476664Z","shell.execute_reply.started":"2023-06-10T07:45:30.466644Z","shell.execute_reply":"2023-06-10T07:45:30.474968Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"dict_keys(['input_ids', 'attention_mask'])\n","output_type":"stream"}]},{"cell_type":"code","source":"model.eval()\nbeam_outputs = model.generate(\n    input_ids=input_ids,attention_mask=attention_mask,\n    max_length=72,\n    early_stopping=True,\n    num_beams=5,\n    num_return_sequences=3\n\n)\n\nfor beam_output in beam_outputs:\n    sent = tokenizer.decode(beam_output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n    print (sent)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T07:45:30.478575Z","iopub.execute_input":"2023-06-10T07:45:30.479054Z","iopub.status.idle":"2023-06-10T07:45:30.801034Z","shell.execute_reply.started":"2023-06-10T07:45:30.479017Z","shell.execute_reply":"2023-06-10T07:45:30.799952Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"question: All departments in the college of science offer what type of degree?\nquestion: All departments in the college of science offer what degree?\nquestion: All departments of the college of science offer what type of degree?\n","output_type":"stream"}]},{"cell_type":"code","source":"# Download the trained weights \nimport shutil\n\nfolder_path = \"/kaggle/working/t5\"  # Path to the folder you want to download\nzip_file_path = \"/kaggle/working/t5\"  # Path to save the zip file\n\nshutil.make_archive(zip_file_path, 'zip', folder_path)","metadata":{"execution":{"iopub.status.busy":"2023-06-10T07:44:03.648296Z","iopub.execute_input":"2023-06-10T07:44:03.648672Z","iopub.status.idle":"2023-06-10T07:44:57.358576Z","shell.execute_reply.started":"2023-06-10T07:44:03.648643Z","shell.execute_reply":"2023-06-10T07:44:57.356875Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/t5.zip'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}